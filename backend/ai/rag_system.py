import os
import uuid
from typing import List, Dict, Any
import chromadb
from chromadb.config import Settings
from pypdf import PdfReader
import requests
import json
from .embeddings import embeddings_model
from dotenv import load_dotenv
import re
import asyncio
import httpx
from bs4 import BeautifulSoup
from urllib.parse import urljoin, urlparse

load_dotenv('../.env')

class RAGSystem:
    def __init__(self):
        # IniÈ›ializeazÄƒ Chroma DB
        self.chroma_path = os.getenv("CHROMA_PERSIST_DIRECTORY", "./data/chroma_db")
        os.makedirs(self.chroma_path, exist_ok=True)
        
        self.client = chromadb.PersistentClient(path=self.chroma_path)
        
        # Ollama settings
        self.ollama_url = os.getenv("OLLAMA_BASE_URL", "http://localhost:11434")
        self.ollama_model = os.getenv("OLLAMA_MODEL", "llama3.2:3b")
    
    def get_or_create_collection(self, municipality_id: str):
        """ObÈ›ine sau creeazÄƒ colecÈ›ia pentru o primÄƒrie"""
        collection_name = f"docs_{municipality_id}"
        try:
            collection = self.client.get_collection(name=collection_name)
        except:
            collection = self.client.create_collection(
                name=collection_name,
                metadata={"municipality": municipality_id}
            )
        return collection
    
    async def process_html_url(self, url: str, municipality_id: str, custom_title: str = None) -> bool:
        """
        ProceseazÄƒ o paginÄƒ HTML de pe orice site web È™i o adaugÄƒ Ã®n baza de vectori
        """
        try:
            print(f"ğŸ“„ Procesez HTML de pe: {url}")
            
            async with httpx.AsyncClient(timeout=30.0) as client:
                response = await client.get(url)
                response.raise_for_status()
                
                soup = BeautifulSoup(response.content, 'html.parser')
                
                # Extrage titlul
                title = custom_title
                if not title:
                    title_tag = soup.find('title')
                    title = title_tag.get_text().strip() if title_tag else f"Document web - {urlparse(url).netloc}"
                
                # CurÄƒÈ›Äƒ HTML-ul
                content_text = self._extract_clean_content(soup, url)
                
                if len(content_text.strip()) < 100:
                    print(f"âš ï¸ ConÈ›inut insuficient pentru {url}")
                    return False
                
                # Ãmparte textul Ã®n chunks
                chunks = self._split_text(content_text, chunk_size=500, overlap=50)
                
                # CreeazÄƒ embeddings
                embeddings = embeddings_model.embed_texts(chunks)
                
                # AdaugÄƒ Ã®n Chroma
                collection = self.get_or_create_collection(municipality_id)
                
                source_name = f"{urlparse(url).netloc}_{title[:50]}"
                ids = [f"{source_name}_{i}_{str(uuid.uuid4())[:8]}" for i in range(len(chunks))]
                metadatas = [
                    {
                        "source": title,
                        "url": url,
                        "source_type": "html",
                        "chunk_id": i,
                        "municipality": municipality_id,
                        "domain": urlparse(url).netloc
                    }
                    for i, chunk in enumerate(chunks)
                ]
                
                collection.add(
                    embeddings=embeddings,
                    documents=chunks,
                    metadatas=metadatas,
                    ids=ids
                )
                
                print(f"âœ… HTML procesat: {title} - {len(chunks)} chunks")
                return True
                
        except Exception as e:
            print(f"âŒ Eroare procesare HTML {url}: {e}")
            return False
    
    async def process_multiple_html_urls(self, urls: List[str], municipality_id: str, titles: List[str] = None) -> Dict[str, Any]:
        """
        ProceseazÄƒ mai multe URL-uri HTML Ã®n paralel
        """
        if titles and len(titles) != len(urls):
            titles = None
            
        tasks = []
        for i, url in enumerate(urls):
            title = titles[i] if titles else None
            tasks.append(self.process_html_url(url, municipality_id, title))
        
        results = await asyncio.gather(*tasks, return_exceptions=True)
        
        successful = 0
        failed = 0
        errors = []
        
        for i, result in enumerate(results):
            if isinstance(result, Exception):
                failed += 1
                errors.append(f"{urls[i]}: {str(result)}")
            elif result:
                successful += 1
            else:
                failed += 1
                errors.append(f"{urls[i]}: ConÈ›inut insuficient")
        
        return {
            "total_urls": len(urls),
            "successful": successful,
            "failed": failed,
            "errors": errors
        }
    
    def _extract_clean_content(self, soup: BeautifulSoup, url: str) -> str:
        """
        Extrage conÈ›inutul curat din HTML, eliminÃ¢nd navigaÈ›ia È™i elementele irelevante
        """
        # EliminÄƒ script-uri, stiluri, È™i alte elemente irelevante
        for tag in soup.find_all(['script', 'style', 'nav', 'header', 'footer', 'aside', 'meta', 'link']):
            tag.decompose()
        
        # EliminÄƒ divuri comune pentru ads È™i social media
        for class_name in ['advertisement', 'ads', 'social', 'share', 'comments', 'sidebar', 'menu']:
            for tag in soup.find_all(attrs={'class': re.compile(class_name, re.I)}):
                tag.decompose()
        
        # ÃncearcÄƒ sÄƒ gÄƒseascÄƒ conÈ›inutul principal
        content_text = ""
        
        # Strategii de extragere Ã®n ordinea prioritÄƒÈ›ii
        content_selectors = [
            'article',
            '.main-content',
            '.content',
            '.post-content',
            '.article-content',
            '.entry-content',
            '#content',
            '#main',
            'main',
            '.container',
            '.wrapper'
        ]
        
        for selector in content_selectors:
            content_element = soup.select_one(selector)
            if content_element:
                content_text = content_element.get_text()
                if len(content_text.strip()) > 200:  # ConÈ›inut substanÈ›ial
                    break
        
        # Fallback - ia tot din body dacÄƒ nu gÄƒseÈ™te conÈ›inut specific
        if not content_text or len(content_text.strip()) < 200:
            body = soup.find('body')
            content_text = body.get_text() if body else soup.get_text()
        
        # CurÄƒÈ›Äƒ textul
        return self._clean_text(content_text, url)
    
    def _clean_text(self, text: str, url: str = "") -> str:
        """
        CurÄƒÈ›Äƒ È™i normalizeazÄƒ textul extras
        """
        # EliminÄƒ caractere speciale È™i normalizeazÄƒ spaÈ›iile
        text = re.sub(r'\s+', ' ', text)
        text = re.sub(r'[^\w\s\-.,;:!?()\/\[\]"\'%â‚¬$]', '', text)
        
        # EliminÄƒ linii foarte scurte (probabil navigaÈ›ie/UI)
        lines = text.split('\n')
        cleaned_lines = []
        
        for line in lines:
            line = line.strip()
            # PÄƒstreazÄƒ doar liniile cu conÈ›inut substanÈ›ial
            if (len(line) > 30 and 
                not line.lower().startswith(('copyright', 'Â©', 'all rights', 'terms', 'privacy', 'cookie')) and
                not re.match(r'^[\s\d\-\.]+$', line)):  # EliminÄƒ linii cu doar numere/punctuaÈ›ie
                cleaned_lines.append(line)
        
        result = '\n'.join(cleaned_lines).strip()
        
        # AdaugÄƒ informaÈ›ii despre sursa web
        if url:
            domain = urlparse(url).netloc
            result = f"Sursa: {domain}\nURL: {url}\n\n{result}"
        
        return result
    
    def process_pdf(self, pdf_path: str, municipality_id: str, filename: str) -> bool:
        """ProceseazÄƒ un PDF È™i Ã®l adaugÄƒ Ã®n baza de vectori"""
        try:
            # CiteÈ™te PDF
            reader = PdfReader(pdf_path)
            full_text = ""
            
            for page_num, page in enumerate(reader.pages):
                text = page.extract_text()
                full_text += f"\n--- Pagina {page_num + 1} ---\n{text}"
            
            # Ãmparte textul Ã®n chunks
            chunks = self._split_text(full_text, chunk_size=500, overlap=50)
            
            # CreeazÄƒ embeddings
            embeddings = embeddings_model.embed_texts(chunks)
            
            # AdaugÄƒ Ã®n Chroma
            collection = self.get_or_create_collection(municipality_id)
            
            ids = [f"{filename}_{i}_{str(uuid.uuid4())[:8]}" for i in range(len(chunks))]
            metadatas = [
                {
                    "source": filename,
                    "source_type": "pdf",
                    "chunk_id": i,
                    "municipality": municipality_id,
                    "page": self._extract_page_from_chunk(chunk)
                }
                for i, chunk in enumerate(chunks)
            ]
            
            collection.add(
                embeddings=embeddings,
                documents=chunks,
                metadatas=metadatas,
                ids=ids
            )
            
            print(f"âœ… PDF procesat: {filename} - {len(chunks)} chunks")
            return True
            
        except Exception as e:
            print(f"âŒ Eroare procesare PDF {filename}: {e}")
            return False
    
    def process_text_content(self, text_content: str, municipality_id: str, title: str, source_type: str = "text") -> bool:
        """
        ProceseazÄƒ conÈ›inut text direct (pentru cazuri cÃ¢nd ai deja textul extras)
        """
        try:
            if len(text_content.strip()) < 100:
                print(f"âš ï¸ ConÈ›inut text insuficient pentru {title}")
                return False
            
            # Ãmparte textul Ã®n chunks
            chunks = self._split_text(text_content, chunk_size=500, overlap=50)
            
            # CreeazÄƒ embeddings
            embeddings = embeddings_model.embed_texts(chunks)
            
            # AdaugÄƒ Ã®n Chroma
            collection = self.get_or_create_collection(municipality_id)
            
            source_name = title.replace(' ', '_')[:50]
            ids = [f"{source_name}_{i}_{str(uuid.uuid4())[:8]}" for i in range(len(chunks))]
            metadatas = [
                {
                    "source": title,
                    "source_type": source_type,
                    "chunk_id": i,
                    "municipality": municipality_id
                }
                for i, chunk in enumerate(chunks)
            ]
            
            collection.add(
                embeddings=embeddings,
                documents=chunks,
                metadatas=metadatas,
                ids=ids
            )
            
            print(f"âœ… Text procesat: {title} - {len(chunks)} chunks")
            return True
            
        except Exception as e:
            print(f"âŒ Eroare procesare text {title}: {e}")
            return False
    
    def search_documents(self, query: str, municipality_id: str, n_results: int = 5) -> List[Dict]:
        """CautÄƒ Ã®n documente rÄƒspunsuri relevante"""
        try:
            collection = self.get_or_create_collection(municipality_id)
            
            # CreeazÄƒ embedding pentru query
            query_embedding = embeddings_model.embed_query(query)
            
            # CautÄƒ Ã®n Chroma
            results = collection.query(
                query_embeddings=[query_embedding],
                n_results=n_results
            )
            
            # FormateazÄƒ rezultatele
            formatted_results = []
            if results['documents'] and results['documents'][0]:
                for i, doc in enumerate(results['documents'][0]):
                    metadata = results['metadatas'][0][i]
                    formatted_results.append({
                        'content': doc,
                        'metadata': metadata,
                        'distance': results['distances'][0][i] if results['distances'] else None,
                        'source_type': metadata.get('source_type', 'unknown'),
                        'url': metadata.get('url', ''),
                        'domain': metadata.get('domain', '')
                    })
            
            return formatted_results
            
        except Exception as e:
            print(f"âŒ Eroare cÄƒutare: {e}")
            return []
    
    def generate_response(self, query: str, context_docs: List[Dict]) -> Dict[str, Any]:
        """GenereazÄƒ rÄƒspuns folosind Ollama cu context Ã®mbunÄƒtÄƒÈ›it"""
        try:
            # ConstruieÈ™te contextul cu informaÈ›ii despre surse
            context = ""
            sources = []
            
            for doc in context_docs:
                source_info = f"{doc['metadata']['source']}"
                
                # AdaugÄƒ informaÈ›ii specifice tipului de sursÄƒ
                if doc['metadata'].get('source_type') == 'html':
                    if doc['metadata'].get('url'):
                        source_info += f" (web: {doc['metadata']['domain']})"
                elif doc['metadata'].get('source_type') == 'pdf':
                    if 'page' in doc['metadata'] and doc['metadata']['page']:
                        source_info += f", pagina {doc['metadata']['page']}"
                
                context += f"\nSursa: {source_info}\nConÈ›inut: {doc['content']}\n"
                sources.append(source_info)
            
            # Prompt Ã®mbunÄƒtÄƒÈ›it pentru model
            prompt = f"""
EÈ™ti un asistent AI specializat Ã®n legislaÈ›ia fiscalÄƒ romÃ¢neascÄƒ pentru primÄƒrii.

Context din documente oficiale (PDF, web, text):
{context}

Ãntrebare: {query}

InstrucÈ›iuni:
1. RÄƒspunde DOAR pe baza informaÈ›iilor din context
2. DacÄƒ nu gÄƒseÈ™ti informaÈ›ia Ã®n context, spune cÄƒ nu ai informaÈ›ia necesarÄƒ
3. CiteazÄƒ sursa informaÈ›iei (document, paginÄƒ, sau site web)
4. RÄƒspunde Ã®n romÃ¢nÄƒ, clear È™i concis
5. FoloseÈ™te un ton oficial dar accesibil
6. DacÄƒ informaÈ›ia vine de pe web, menÈ›ioneazÄƒ cÄƒ este de pe site-ul oficial

RÄƒspuns:
"""

            # Apel la Ollama
            response = requests.post(
                f"{self.ollama_url}/api/generate",
                json={
                    "model": self.ollama_model,
                    "prompt": prompt,
                    "stream": False,
                    "options": {
                        "temperature": 0.3,
                        "num_predict": 400
                    }
                }
            )
            
            if response.status_code == 200:
                result = response.json()
                return {
                    "response": result['response'].strip(),
                    "sources": list(set(sources)),
                    "context_used": len(context_docs),
                    "source_types": list(set([doc['metadata'].get('source_type', 'unknown') for doc in context_docs]))
                }
            else:
                return {
                    "response": "Scuze, am Ã®ntÃ¢mpinat o problemÄƒ tehnicÄƒ. ÃncercaÈ›i din nou.",
                    "sources": [],
                    "context_used": 0,
                    "source_types": []
                }
                
        except Exception as e:
            print(f"âŒ Eroare generare rÄƒspuns: {e}")
            return {
                "response": "Scuze, am Ã®ntÃ¢mpinat o problemÄƒ tehnicÄƒ. ÃncercaÈ›i din nou.",
                "sources": [],
                "context_used": 0,
                "source_types": []
            }
    
    def list_documents(self, municipality_id: str) -> List[Dict]:
        """
        ListeazÄƒ toate documentele procesate pentru o primÄƒrie
        """
        try:
            collection = self.get_or_create_collection(municipality_id)
            
            # ObÈ›ine toate documentele
            all_docs = collection.get()
            
            # GrupeazÄƒ dupÄƒ sursÄƒ pentru a evita duplicatele
            sources = {}
            for i, metadata in enumerate(all_docs['metadatas']):
                source_key = metadata['source']
                if source_key not in sources:
                    sources[source_key] = {
                        'source': metadata['source'],
                        'source_type': metadata.get('source_type', 'unknown'),
                        'url': metadata.get('url', ''),
                        'domain': metadata.get('domain', ''),
                        'chunk_count': 0
                    }
                sources[source_key]['chunk_count'] += 1
            
            return list(sources.values())
            
        except Exception as e:
            print(f"âŒ Eroare listare documente: {e}")
            return []
    
    def _split_text(self, text: str, chunk_size: int = 500, overlap: int = 50) -> List[str]:
        """Ãmparte textul Ã®n chunks cu overlap"""
        words = text.split()
        chunks = []
        
        for i in range(0, len(words), chunk_size - overlap):
            chunk = ' '.join(words[i:i + chunk_size])
            if chunk.strip():
                chunks.append(chunk.strip())
                
        return chunks
    
    def _extract_page_from_chunk(self, chunk: str) -> str:
        """Extrage numÄƒrul paginii din chunk"""
        page_match = re.search(r'--- Pagina (\d+) ---', chunk)
        return page_match.group(1) if page_match else ""

# Singleton instance
rag_system = RAGSystem()